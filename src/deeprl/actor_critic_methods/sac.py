"""
TODO 3.9
Generic Alias Type and PEP 585.

TODO
Proper type hint for functools.partial.

TODO
How to understand differential entropy can be negative?
https://en.wikipedia.org/wiki/Entropy_(information_theory)#Differential_entropy
"""

from copy import deepcopy
from functools import partial
from itertools import chain
from typing import Callable, Iterable, Iterator, List, Optional, Protocol

import torch
import torch.nn as nn
import torch.nn.functional as F
from attrs import define
from cytoolz import comp
from cytoolz.curried import map, reduce
from torch import Tensor, add, min
from torch.distributions import Distribution
from torch.nn.parameter import Parameter
from torch.optim import Optimizer

from .experience_replay import Batch


class ExperienceReplay(Protocol):
    def push(
        self,
        state: Tensor,
        action: Tensor,
        reward: Tensor,
        next_state: Tensor,
        terminated: Tensor,
    ) -> None:
        ...

    def sample(self, batch_size: int) -> Batch:
        ...


@define
class SAC:
    """Soft Actor-Critic"""

    _policy: nn.Module
    _qualities: List[nn.Module]
    _log_temperature: Tensor
    _target_qualities: List[nn.Module]
    _target_entropy: float
    _policy_optimiser: Optimizer
    _quality_optimiser: Optimizer
    _temperature_optimiser: Optimizer
    _experience_replay: ExperienceReplay
    _batch_size: int
    _discount_factor: float
    _polyak_factor: float

    def _update_parameters(self) -> None:
        try:
            batch = self._experience_replay.sample(self._batch_size)
        except ValueError:
            return

        # Abbreviating to mathematical italic unicode char for readability
         = batch.states
         = batch.actions
         = batch.rewards
        始 = batch.next_states
         = batch.terminateds
         = self._discount_factor
        _ = self._qualities
        始_ = self._target_qualities
         = self._polyak_factor
        log = self._log_temperature
         = log.exp().detach()
         = self._target_entropy

        # Compute target action and its log-likelihood
        始: Distribution = self._policy(始)
        ⑹ = 始.rsample()  # Reparameterised sample
        log始: Tensor = 始.log_prob(⑹)
        log始 = log始.sum(dim=1, keepdim=True)  # Sum log prob of multiple actions

         =  + ~ *  * (min(*[始(始, ⑹) for 始 in 始_]) -  * log始)
        action_quality = [(, ) for  in _]
        quality_loss_fn = comp(reduce(add), map(partial(F.mse_loss, target=)))
        quality_loss: Tensor = quality_loss_fn(action_quality)
        self._quality_optimiser.zero_grad()
        quality_loss.backward()
        self._quality_optimiser.step()

        # Compute action and its log-likelihood
        : Distribution = self._policy()
        茫 = .rsample()
        log: Tensor = .log_prob(茫)
        log = log.sum(dim=1, keepdim=True)

        policy_loss = ( * log - min(*[(, 茫) for  in _])).mean()
        self._policy_optimiser.zero_grad()
        policy_loss.backward()
        self._policy_optimiser.step()

        temperature_loss = (-log * (log.detach() + )).mean()
        self._temperature_optimiser.zero_grad()
        temperature_loss.backward()
        self._temperature_optimiser.step()

        # Update frozen target quality fn approximators by Polyak averaging (exponential smoothing)
        with torch.no_grad():
            for , 始 in zip(_, 始_):
                for , 始 in zip(.parameters(), 始.parameters()):
                    始.copy_( *  + (1.0 - ) * 始)

    @torch.no_grad()
    def compute_action(self, state: Tensor) -> Tensor:
        return self._policy(state).rsample()

    def step(
        self,
        state: Tensor,
        action: Tensor,
        reward: Tensor,
        next_state: Tensor,
        terminated: Tensor,
    ) -> None:
        self._experience_replay.push(state, action, reward, next_state, terminated)
        self._update_parameters()

    @classmethod
    def init(
        cls,
        policy: nn.Module,
        quality: nn.Module,
        policy_optimiser_init: Callable[[Iterator[Parameter]], Optimizer],
        quality_optimiser_init: Callable[[Iterator[Parameter]], Optimizer],
        temperature_optimiser_init: Callable[[Iterable[Tensor]], Optimizer],
        experience_replay: ExperienceReplay,
        batch_size: int,
        discount_factor: float,
        target_entropy: float,
        polyak_factor: float,  # Exponential smoothing
        num_qualities: int = 2,
        device: Optional[torch.device] = None,
    ) -> "SAC":
        policy = policy.to(device)
        qualities = [deepcopy(quality.to(device)) for _ in range(num_qualities)]
        # TODO: Why using log value of temperature in temperature loss are generally nicer?
        # https://github.com/toshikwa/soft-actor-critic.pytorch/issues/2
        log_temperature = torch.zeros(1, requires_grad=True, device=device)

        policy_optimiser = policy_optimiser_init(policy.parameters())
        quality_optimiser = quality_optimiser_init(
            chain(*[quality.parameters() for quality in qualities])
        )
        temperature_optimiser = temperature_optimiser_init([log_temperature])

        target_qualities = deepcopy(qualities)
        # Freeze target quality networks with respect to optimisers (only update via Polyak averaging)
        [net.requires_grad_(False) for net in target_qualities]

        return cls(
            policy,
            qualities,
            log_temperature,
            target_qualities,
            target_entropy,
            policy_optimiser,
            quality_optimiser,
            temperature_optimiser,
            experience_replay,
            batch_size,
            discount_factor,
            polyak_factor,
        )
